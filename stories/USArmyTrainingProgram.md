## US Army Training Program: An Agile Success Story

By the mid-2000s, the program that the US Army used for training was coming apart at the seams. More than a decade old at the time, the Automated Systems Approach to Training (ASAT) relied on old technology, and despite a slew of add-ons, patches, and workarounds over the years, the program couldn’t keep up with training needs, delivered inconsistent instruction, contained redundancies, and was expen- sive to maintain.

To replace ASAT, the Army decided to develop a
new system, the Training and Doctrine Development Capability (TDDC), which would ostensibly be state of the art. This plan didn’t quite work out as hoped. While the TDDC was designed to take advantage of the Web and of gains in hardware capabilities, the program’s builders weren’t as forward thinking in their method- ologies. Structured primarily around traditional water- fall development techniques, the project continued
for several years and chewed through nearly $100 million. However, the resulting program never worked to anyone’s satisfaction. It lacked the basic functionality users wanted and it couldn’t handle even a minimum number of concurrent training professionals.

Implementation delays would have meant that users had to endure ASAT for a while longer, but the Army chose to scrap the TDDC entirely and replace it.
This time, Army technologists were determined to
try a different approach. A requirement of the newly drawn up Request for Proposals for the new Training Development Capability (TDC) was that the contractor use agile methodology, with collaborative teams, frequent iterations, constant load testing, and deep engagement by the user community. A fully working product was completed by 2008, less than 2 years
after the project’s start—and there have been no hiccups. The system has been successfully rolled out to all of the Army training schools as a replacement for the ASAT system.

“Because of the  rst  asco with TDDC, I came to
the initial 30-day evaluation of TDC ready to fail it quickly and take an early  ight home,” says Henry Koelzer, a retired artillery NCO and early evaluator
of the project. But after just a few hours, he decided, “This system, and the agile programming methodology, was going to work.”

The primary failing of ASAT was its dependence on 1990s two-tiered, fat client architecture, which resulted in a wholly decentralized program. “Every school was
a system in itself,” says Dennis Baston, who is retired now but was as a Supervisory Systems Analyst at the US Army Training Support Center.

For example, the training software used at Fort Knox’s armor school, Fort Benning’s infantry school, and Fort Sill’s  eld artillery school had to be loaded manually on servers at each of these locations. And because the applications were stovepiped, the installations at the separate schools could not practically communicate with each other. The sheer redundancy of the course- ware and the need to dedicate as many as 50 different servers exclusively to ASAT was a huge a drain on tech- nology and  nancial resources.

What’s more, once a course was placed on the server, individual trainers at each school could tweak it to  t their perceived needs. As a result, there were multiple versions of each set of training materials  oating around, and no way of knowing which was the most current. In fact, sometimes a course got so lost in the system that it could only be found with an extensive search—and lots of manpower earmarked for it. At the Fort Knox armor school, for instance, after a search for the most current version of the weapons maintenance course, Army training professionals  nally found it in the music school. “Who would have guessed those people were so hard core,” Baston says.

Baston adds that when congressional investigators and US prosecutors asked to see the training content related to interrogation methods used at Iraq’s Abu Ghraib prison, after military personnel were found
to abuse inmates there in 2003, it was impossible to de nitively decipher which version each soldier actually received.

Consequently, the Army’s goal in developing TDDC (and later TDC) was to provide an integrated and centralized repository of training products that were approved, under development, or being considered for general use. In addition, secondary bene ts sought included eliminating the duplicate content and reduce the time to develop training products.

The contract to build the TDDC was awarded to what Koelzer calls “a major company; one you would imme- diately recognize.” Despite the vendor’s reputation and resources, the waterfall approach doomed the project from the start.
Following the typical waterfall techniques, program requirements were set in stone during the planning phase even before one line of code was written.
No Army users—trainers or trainees—saw the inter- faces and tested the functionality until TDDC was completed and delivered. “We gave them the use case, function points, and other major speci cations, and when they were all done, they gave us the soft- ware, which was going to be a surprise, either good or bad,” recalls Baston.

The contractor tried to minimize the risk of the water- fall method by pairing it with spiral development techniques, which involves more testing and even agile-like iterations during the project, but the spiral model shares a fatal  aw with the waterfall model: the program’s requirements cannot change during develop- ment. So government evaluators were uncertain what they would see when the product was  nally delivered. Combine that with the spiral approach of working on overlapping aspects of the project at the same time, with these separate mini-development teams basing their activities on user requirements, functions, and
features that were frozen in time during upfront plan- ning. “So what you end up with is organized chaos,” Baston says.

But even given that the waterfall method doesn’t allow for modi cations in project design, Baston says, “What was delivered didn’t meet the requirements that were speci ed in the  rst place.” He attributes this result
to the fact that he and other evaluators could not see, and make corrections to, what was being produced until the very end.

For example, the system was supposed to support 6,000 training developers. But the software couldn’t handle a load anywhere close to that, perhaps fewer than 100. Baston pins the blame on the contractor’s testing process. Rather than assessing the system with real developers and in realistic numbers of concurrent users, the contractors used a few of its own coders and not in suf cient numbers to push the software to the breaking point.

The outcome couldn’t have been more of a disaster. After carefully evaluating the TDDC, Baston determined with 98-percent certainty that it could not be fixed and should be shelved. However, the project lead, a two-star general who was the deputy chief of staff for operations, was not willing to trash such an expensive effort even though Baston had given it a 2-percent chance of being  xable. Says Baston, “He wanted more certainty in our  ndings. So we had to go back and do more testing, more in-depth analysis, and we ended up with a 100-percent certainty that it was a complete, unrecoverable failure.”

The project was then rebid, this time as an agile development effort. Phase 1 of the TDC, which began November 1, 2006, was a 30-day demonstration phase, at the end of which the prospective contractors had to demonstrate a prototype to a packed house of about 30 government evaluators. On the basis of this session, the contract was awarded to the contractor team of Unitech, Booz Allen Hamilton, and MPRI.

One immediate advantage of agile methodology over the waterfall approach was its continuous performance testing regime even during the development of the soft- ware. For example, load measurements were conducted each month with an application that estimated total system capability based on the behavior of the program when accessed by a large number of concurrent users, as many as 30,000 by the time the software was ready to launch. In addition to merely issuing a “yea” or “nay” to do it at no cost. However, when it went beyond a small adjustment, the Army and the contractors negoti- ated ways to put more resources into that area of the system while streamlining other sections.

For example, the military had failed to include a critical security function in its system requirements. When that substantial shortcoming became evident, the develop- ment team and the government hammered out ways to make up for it, eventually agreeing to reuse some of the existing system accreditation documentation from the earlier programs. That freed up resources to tackle the security gap.

In the end the project came in on budget and on time.

“What I saw happening was that there was an acceptance of the system from the user base as opposed to the contractor having to try to force its  nished results on people,” says Baston.

The final phase of the project, including deployment, maintenance, and data conversion, lasted from July 1, 2008, through September 30, 2008, when the training system went live.

The project was successfully deployed, and now training professionals can to access courseware via a Web browser and use the portions of it that they need without corrupting the original program. As new content is added to the courseware templates, the system keeps track of which version is the most recent and who is responsible for it. When an appropriate super- visor signs off on a new version, the updated training materials are marked as complete and are made avail- able to anyone with TDC access.

TDC has already generated numerous critical improve- ments with tangible gains. Perhaps the single largest bene t is TDC’s impact on the preparation of course description documentation—known as Course Administrative Data (CAD) and Program of Instruction (POI)—which ultimately determine funding for training efforts. Accuracy is essential, so each CAD or POI undergoes a lengthy review by  nancial, training, and training development experts at Training Operations Management Authority (TOMA) before submission
to the Department of the Army. With ASAT, schools submitted these documents by exporting their data- bases to hard drives, which were then mailed to TOMA. In turn, TOMA personnel would import the data onto their servers, indicate necessary changes, and then
send the edited document back to the schools. The schools would make the required corrections, and the process would begin all over again. This system was so cumbersome that TOMA could barely meet the Army’s minimum requirements for training assessment.

In sharp contrast, under TDC, CAD and POI are sent to TOMA through the work ow architecture within the system. TOMA receives noti cation electronically, and its experts then make comments directly into the  les and route them back to the school for changes. As a result, submission times to the Army for CAD and POI have been reduced from about 1 month under ASAT to 1 day under TDC.

In addition TDC’s security architecture permits compart- mentalization of information not possible under ASAT. With ASAT, restricting which information each user
had access to was a complicated process. As a result, sometimes unauthorized users would inadvertently
edit or change a  le that didn’t belong to them. By providing  ve separate domains, TDC allows supervi- sors to limit user access to only those programs they’re authorized to work on.
TDC also allows for consolidation of equipment, which reduces hardware, support and security costs, and complexity. ASAT ran on 78 different servers, each of which had to be housed in a restricted physical loca- tion. TDC runs on just a handful of web servers and a single database server.

Currently, TDC is used by almost 3,000 people on a daily basis and intermittently by an additional 3,000 users.

##### For More Information
* Shawn M. Faunce, faunce_shawn@bah.com - Booz Allen Hamilton
* Dan Tucker, tucker_dan@bah.com - Booz Allen Hamilton
* Haluk Saker, saker_haluk@bah.com - Booz Allen Hamilton
* Wyatt Chaffee, chaffee_wyatt@bah.com - Booz Allen Hamilton
